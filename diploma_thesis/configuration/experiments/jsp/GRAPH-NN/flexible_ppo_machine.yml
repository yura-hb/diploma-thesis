
kind: 'rl'
parameters:

  model:
    kind: 'deep_multi_rule'
    parameters:

      policy:
        kind: 'flexible_action'
        parameters:
          policy_method: 'independent'

          model:
            __model__: ''

          action_selector:
            kind: 'phase_selector'
            parameters:
              default:
                kind: 'sample'
                parameters:
                  is_distribution: False
              phases:
                - phase:
                    kind: 'warm_up'
                    parameters:
                      step: 0
                  action_selector:
                    kind: 'uniform'
                - phase:
                    kind: 'warm_up'
                    parameters:
                      step: 1
                  action_selector:
                    kind: 'uniform'
                - phase:
                    kind: 'training'
                  action_selector:
                    kind: 'sample'
                    parameters:
                      is_distribution: False


  __encoder__: ''

  trainer:
    kind: 'ppo'
    parameters:
      device: 'cpu'
      sample_count: 256
      policy_step_ratio: 0.2
      entropy_regularization: 0.0001
      entropy_decay: 0.999
      rollback_ratio: 0.001
      critic_weight: 0.5

      epochs: 1

      loss:
        kind: 'cross_entropy'
        parameters:
          reduction: 'none'

      value_loss:
        kind: 'mse'

      optimizer:
        model:
          kind: 'adam_w'
          parameters:
            lr: 0.001

      memory:
        kind: 'replay'
        parameters:
          size: 8
          batch_size: 8
          prefetch: 1

      return:
        kind: 'gae'
        parameters:
          discount_factor: 0.99
          lambda: 0.95